# Databricks notebook source
# MAGIC %md ## Serving Models with Microsoft Azure ML
# MAGIC 
# MAGIC ##### NOTE: I do not recommend using *Run All* because it takes several minutes to deploy and update models; models cannot be queried until they are active.

# COMMAND ----------

# MAGIC %md ### Create or load an Azure ML Workspace

# COMMAND ----------

# MAGIC %md Before models can be deployed to Azure ML, you must create or obtain an Azure ML Workspace. The `azureml.core.Workspace.create()` function will load a workspace of a specified name or create one if it does not already exist. For more information about creating an Azure ML Workspace, see the [Azure ML Workspace management documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace).

# COMMAND ----------

import mlflow
import azureml.mlflow
from azureml.core import Workspace
from azureml.core.authentication import ServicePrincipalAuthentication
from mlflow.deployments import get_deploy_client
from mlflow.tracking.client import MlflowClient

workspace_name = dbutils.secrets.get(scope = "azureml", key = "workspace_name")
workspace_location = "eastus"
resource_group = dbutils.secrets.get(scope = "azureml", key = "resource_group")
subscription_id = dbutils.secrets.get(scope = "azureml", key = "subscription_id")

svc_pr = ServicePrincipalAuthentication(
    tenant_id = dbutils.secrets.get(scope = "azureml", key = "tenant_id"),
    service_principal_id = dbutils.secrets.get(scope = "azureml", key = "client_id"),
    service_principal_password = dbutils.secrets.get(scope = "azureml", key = "client_secret"))

workspace = Workspace.create(name = workspace_name,
                             location = workspace_location,
                             resource_group = resource_group,
                             subscription_id = subscription_id,
                             auth=svc_pr,
                             exist_ok=True)

azureml_mlflow_uri = f"azureml://{workspace_location}.api.azureml.ms/mlflow/v1.0/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}"
mlflow.set_tracking_uri(workspace.get_mlflow_tracking_uri())
client = get_deploy_client(mlflow.get_tracking_uri())

# COMMAND ----------

# MAGIC %md ###Get the Model from Experiment

# COMMAND ----------

from mlflow.tracking.client import MlflowClient
from mlflow.entities import ViewType

experiment = MlflowClient().get_experiment_by_name('WineQuality')

print("Experiment ID:", experiment.experiment_id)

query = "metrics.rmse < 0.8"
runs = MlflowClient().search_runs(experiment.experiment_id, query, ViewType.ALL)

rmse_low = None
run_id = None
for run in runs:
  if (rmse_low == None or run.data.metrics['rmse'] < rmse_low):
    rmse_low = run.data.metrics['rmse']
    run_id = run.info.run_id
print("Lowest RMSE:", rmse_low)
print("Run ID:", run_id)

model_uri = "runs:/" + run_id+ "/model"

# COMMAND ----------

# MAGIC %md ## Deploy the model to Azure ML Real-Time End Point

# COMMAND ----------

import json
from mlflow.deployments import get_deploy_client

# Create the deployment configuration.
# If no deployment configuration is provided, then the deployment happens on ACI.
deploy_config = {
    "instance_type": "Standard_DS2_v2",
    "instance_count": 1,
}

# Write the deployment configuration into a file.
deployment_config_path = "/tmp/deployment_config.json"
with open(deployment_config_path, "w") as outfile:
    outfile.write(json.dumps(deploy_config))


# MLflow requires the deployment configuration to be passed as a dictionary.
config = {"deploy-config-file": deployment_config_path}

# define the model path and the name is the service name
# if model is not registered, it gets registered automatically and a name is autogenerated using the "name" parameter below
client.create_deployment(
    model_uri=model_uri,
    config=config,
    name="winequality-model",
    endpoint='winequality-endpoint'
)
